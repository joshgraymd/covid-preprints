{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A shift of editorial focus towards dissemination of information on the SARS-CoV-2 pandemic #\n",
    "\n",
    "This notebook contains the scripts necessary to reproducibly create the data analyzed in Gray et al. 2020.\n",
    "\n",
    "## Sections:\n",
    "\n",
    "1. Obtain DOIs of papers pertain to the COVID-19 pandemic.\n",
    "2. Construct the three cohorts of version 1 (v1) preprint abstracts: \n",
    "    * The x months leading up to the beginning of the pandemic<sup>*</sup>\n",
    "    * The SARS-CoV-2 COVID-19 articles\n",
    "    * Articles deposited as preprints and published in the course of the pandemic\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES = 5\n",
    "PUBMED_API_KEY = \"cca9551545bbf746ce6248903f91daed6b08\"\n",
    "import json\n",
    "\n",
    "with open('data.json', 'r') as f:\n",
    "    dataWithPublished = json.load(f)\n",
    "\n",
    "with open('metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {},
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 297/297 [01:51<00:00,  2.67it/s]\n8907 COVID-19 SARS-CoV-2 DOIs found\n\n"
    }
   ],
   "source": [
    "# Use the bioRxiv API to obtain DOIs of papers pertaining to the COVID-19 pandemic\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import requests\n",
    "\n",
    "covid_collection_base = 'https://api.biorxiv.org/covid19/%d'\n",
    "\n",
    "covid_preprints = []\n",
    "while True:\n",
    "    try:\n",
    "        lengthInfo = json.loads(requests.get(covid_collection_base % 0).text)\n",
    "        totalDocs = lengthInfo['messages'][0]['total']\n",
    "        stepInterval = lengthInfo['messages'][0]['count']\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "for i in tqdm(range(0, totalDocs, stepInterval)):\n",
    "    retries = 0\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            response = json.loads(requests.get(covid_collection_base % i).text)\n",
    "            covid_preprints = covid_preprints + [{'server': datum['rel_site'], 'DOI': datum['rel_doi']} for datum in response['collection']]\n",
    "            break\n",
    "        except:\n",
    "            retries += 1\n",
    "print('\\n%s COVID-19 SARS-CoV-2 DOIs found' % len(covid_preprints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "end_execution_time": "2020-09-13T00:01:02.062Z",
     "start_execution_time": "2020-09-12T23:03:01.000Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1339/1339 [56:05<00:00,  2.51s/it]\n100%|██████████| 146/146 [01:15<00:00,  1.95it/s]\n100%|██████████| 148402/148402 [00:01<00:00, 111648.03it/s]\n"
    }
   ],
   "source": [
    "# Get information for all articles in bioRxiv and medRxiv\n",
    "from datetime import datetime\n",
    "\n",
    "data = []\n",
    "while True:\n",
    "    try:\n",
    "        date_1st_article_mRxiv = datetime.strptime(json.loads(requests.get('http://api.biorxiv.org/details/medrxiv/1').text)['collection'][0]['date'],'%Y-%m-%d').date()\n",
    "        date_1st_article_bRxiv = datetime.strptime(json.loads(requests.get('http://api.biorxiv.org/details/biorxiv/1').text)['collection'][0]['date'],'%Y-%m-%d').date()\n",
    "        medRxivLengthResponse = json.loads(requests.get('http://api.biorxiv.org/details/medrxiv/%s/%s/%s' % (date_1st_article_mRxiv,datetime.now().date(), 0), timeout=100).text)['messages'][0]\n",
    "        bioRxivLengthResponse = json.loads(requests.get('http://api.biorxiv.org/details/biorxiv/%s/%s/%s' % (date_1st_article_bRxiv,datetime.now().date(), 0), timeout=100).text)['messages'][0]\n",
    "        break\n",
    "    except:\n",
    "        pass\n",
    "for cursor in tqdm(range(0, bioRxivLengthResponse['total'], bioRxivLengthResponse['count'])):\n",
    "    retries = 0\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            response = json.loads(requests.get('http://api.biorxiv.org/details/biorxiv/%s/%s/%s' % (date_1st_article_bRxiv,datetime.now().date(), cursor), timeout=100).text)\n",
    "            break\n",
    "        except:\n",
    "            retries += 1\n",
    "    data += response['collection']\n",
    "for cursor in tqdm(range(0, medRxivLengthResponse['total'], medRxivLengthResponse['count'])):\n",
    "    retries = 0\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            response = json.loads(requests.get('http://api.biorxiv.org/details/medrxiv/%s/%s/%s' % (date_1st_article_mRxiv,datetime.now().date(), cursor), timeout=100).text)\n",
    "            break\n",
    "        except:\n",
    "            retries += 1\n",
    "    data += response['collection']\n",
    "\n",
    "# Computing derived preprint properties\n",
    "for entry in tqdm(data):\n",
    "    if 'authors' in entry:\n",
    "        if len(entry['authors']) > 0 and entry['authors'][-1] == ';':\n",
    "            entry['pp_num_authors'] = len(entry['authors'][:-1].split(';'))\n",
    "        else:\n",
    "            entry['pp_num_authors'] = len(entry['authors'].split(';'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedDataWithPublished = [x for x in data if 'published' in x and x['published'] != 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 63955/63955 [25:39<00:00, 41.55it/s]\n"
    }
   ],
   "source": [
    "for entry in tqdm(updatedDataWithPublished):\n",
    "    alreadyIndexedDOI = next((x for x in dataWithPublished if x['doi'] == entry['doi']), None)\n",
    "    if alreadyIndexedDOI is not None:\n",
    "        latestPulledVersion = max([int(x['version']) for x in dataWithPublished if x['doi'] == entry['doi']])\n",
    "        if int(entry['version']) > latestPulledVersion:\n",
    "            dataWithPublished.append(entry)   \n",
    "    else:\n",
    "        dataWithPublished.append(entry)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 63954/63954 [00:00<00:00, 914570.18it/s]\n100%|██████████| 63954/63954 [42:12<00:00, 25.26it/s]\n\nFound 58687 entries with a PMID\nFound 45402 entires with a PMCID\n\n"
    }
   ],
   "source": [
    "# Get data from pubmed for peer-reviewed titles and abstracts\n",
    "from eutils import Client\n",
    "\n",
    "ec = Client(api_key=PUBMED_API_KEY)\n",
    "doisProcessed = {}\n",
    "for entry in tqdm(dataWithPublished):\n",
    "    if 'pmid' in entry and entry['pmid'] is not None:\n",
    "        doisProcessed[entry['doi']] = {}\n",
    "        doisProcessed[entry['doi']]['pmid'] = entry['pmid']\n",
    "        doisProcessed[entry['doi']]['pmcid'] = entry['pmcid']\n",
    "\n",
    "\n",
    "# Gather all possible PMIDs from pubmed\n",
    "for entry in tqdm(dataWithPublished):\n",
    "    if 'pmid' in entry and entry['pmid'] is not None:\n",
    "        continue\n",
    "    entry['pmid'] = None\n",
    "    entry['pmcid'] = None\n",
    "    doi = entry['published']\n",
    "    retries = 0\n",
    "\n",
    "    if doi in doisProcessed:\n",
    "        entry['pmid'] = doisProcessed[doi]['pmid']\n",
    "        entry['pmcid'] = doisProcessed[doi]['pmcid']\n",
    "        continue\n",
    "    doisProcessed[doi] = {}\n",
    "    doisProcessed[doi]['pmid'] = None\n",
    "    doisProcessed[doi]['pmcid'] = None\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            esr = ec.esearch(db='pubmed', term='%s[Location ID]' % doi)\n",
    "            if len(esr.ids) == 1:\n",
    "                sleep(0.5)\n",
    "                paset = ec.efetch(db='pubmed', id=esr.ids)\n",
    "                confirmed = False\n",
    "                pmcid = None\n",
    "                for pa in paset:\n",
    "                    if pa.doi == doi:\n",
    "                        confirmed = True\n",
    "                        pmcid = pa.pmc\n",
    "                if confirmed:\n",
    "                    entry['pmid'] = esr.ids[0]\n",
    "                    entry['pmcid'] = pmcid\n",
    "                    doisProcessed[doi]['pmid'] = esr.ids[0]\n",
    "                    doisProcessed[doi]['pmcid'] = pmcid\n",
    "            break\n",
    "        except:\n",
    "            print(\"error, retrying for doi: %s\" % doi)\n",
    "            retries += 1\n",
    "\n",
    "print(\"\\n\\nFound %d entries with a PMID\" % sum(entry.get('pmid') is not None for entry in dataWithPublished))\n",
    "print(\"Found %d entries with a PMCID\" % sum(entry.get('pmcid') is not None for entry in dataWithPublished))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeAccessInternalElement(element, elementsInPath):\n",
    "    currElement = element\n",
    "    for e in elementsInPath:\n",
    "        currElement = currElement.find(e)\n",
    "        if currElement == None:\n",
    "            return None\n",
    "    return currElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeGetFromJSON(jsonToSearch, pathToSearch):\n",
    "    head = jsonToSearch\n",
    "    for el in pathToSearch:\n",
    "        if el in head:\n",
    "            head = head[el]\n",
    "        else:\n",
    "            return None\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursiveGetTextFromNode(node, nodeToCheckForLabel = \"nonsense\", labelName=\"nonsense\"):\n",
    "    text = \"\"\n",
    "    for elem in node.iter():\n",
    "        if elem.tag == nodeToCheckForLabel:\n",
    "            if labelName in elem.attrib:\n",
    "                if elem.attrib[labelName]:\n",
    "                    text += elem.attrib[labelName] + \" \"\n",
    "        if elem.text:\n",
    "            text += elem.text\n",
    "        if elem.tail: \n",
    "            text += elem.tail + \" \"\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 63954/63954 [00:13<00:00, 4844.79it/s]\n"
    }
   ],
   "source": [
    "from xml.etree import ElementTree\n",
    "\n",
    "for entry in tqdm(dataWithPublished):\n",
    "    pmid = entry['pmid']\n",
    "    doi = entry['published']\n",
    "    if pmid is None:\n",
    "        continue\n",
    "    if 'pub_journal_name' in entry or 'pub_date_accepted' in entry:\n",
    "        continue # Already obtained results on pubmed for this entry\n",
    "    while True:\n",
    "        try:\n",
    "            root = ElementTree.fromstring(requests.get('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id=%s&retmode=xml&api_key=%s' % (pmid, PUBMED_API_KEY), timeout=100).content)\n",
    "            title = safeAccessInternalElement(root, ['PubmedArticle', 'MedlineCitation', 'Article', 'ArticleTitle'])\n",
    "            abstract = safeAccessInternalElement(root, ['PubmedArticle', 'MedlineCitation', 'Article', 'Abstract'])\n",
    "            journalName = safeAccessInternalElement(root, ['PubmedArticle', 'MedlineCitation', 'Article', 'Journal','Title'])\n",
    "            authorList = safeAccessInternalElement(root, ['PubmedArticle', 'MedlineCitation', 'Article', 'AuthorList'])\n",
    "            pubHistory = safeAccessInternalElement(root, ['PubmedArticle', 'PubmedData', 'History'])\n",
    "            if pubHistory is not None:\n",
    "                for dateEntry in pubHistory.findall('PubMedPubDate'):\n",
    "                    entry['pub_date_%s' % (dateEntry.attrib['PubStatus'] if 'PubStatus' in dateEntry.attrib else 'undefined')] = datetime(year=int(dateEntry.find('Year').text), month=int(dateEntry.find('Month').text), day=(int(dateEntry.find('Day').text) if dateEntry.find('Day') is not None else 1))\n",
    "            if journalName is not None:\n",
    "                entry['pub_journal_name'] = recursiveGetTextFromNode(journalName)\n",
    "            if authorList is not None:\n",
    "                entry['pub_num_authors'] = len(authorList.findall('Author'))\n",
    "            if title is not None:\n",
    "                entry['pub_title'] = recursiveGetTextFromNode(title)\n",
    "            if abstract is not None:\n",
    "                entry['pub_abstract'] = \"\"\n",
    "                for abstractTextElem in abstract.findall('AbstractText'):\n",
    "                    entry['pub_abstract'] += recursiveGetTextFromNode(abstractTextElem, \"AbstractText\", \"Label\")\n",
    "            break\n",
    "        except:\n",
    "            print('error here, sleeping')\n",
    "            sleep(1)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 63954/63954 [00:00<00:00, 1175929.78it/s]\n100%|██████████| 63954/63954 [01:27<00:00, 731.35it/s]\n"
    }
   ],
   "source": [
    "# Gather engagement metrics from the preprint servers\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timezone\n",
    "currentMonth = int(datetime.now().replace(tzinfo=timezone.utc).strftime(\"%m\"))\n",
    "currentMonthStartTimestamp = datetime.strptime(datetime.now().strftime(\"%m/%Y\"), \"%m/%Y\").replace(tzinfo=timezone.utc).timestamp()\n",
    "\n",
    "if str(int(currentMonthStartTimestamp)) not in metadata['engagement']:\n",
    "    metadata['engagement'][str(int(currentMonthStartTimestamp))] = []\n",
    "\n",
    "\n",
    "doisProcessed = {}\n",
    "for entry in tqdm(dataWithPublished):\n",
    "    if 'engagementMetrics' in entry:\n",
    "        doisProcessed[entry['doi']] = entry['engagementMetrics']\n",
    "\n",
    "for entry in tqdm(dataWithPublished):\n",
    "    if entry['doi'] in metadata['engagement'][str(int(currentMonthStartTimestamp))]:\n",
    "        continue\n",
    "    retries = 0\n",
    "    preprintDOI = entry['doi']\n",
    "    entry['engagementMetrics'] = {}\n",
    "    if preprintDOI in doisProcessed:\n",
    "        entry['engagementMetrics'] = doisProcessed[preprintDOI]\n",
    "        continue\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            r = requests.get('https://www.doi.org/%s' % preprintDOI, timeout=100)\n",
    "            if r.status_code != 200:\n",
    "                break\n",
    "            resolvedURL = r.url\n",
    "            metricsRes = requests.get('%s.article-metrics' % resolvedURL, timeout=100)\n",
    "            metricSoup = BeautifulSoup(metricsRes.content, 'html.parser')\n",
    "            numMetrics = len(metricSoup.select('#highwire-highwire-stats-filter-form > div > table > thead > tr > th')) - 1\n",
    "            for i in range(2, numMetrics + 2):\n",
    "                sectionTitle = metricSoup.select('#highwire-highwire-stats-filter-form > div > table > thead > tr > th:nth-child(%d)' % i)[0].text\n",
    "                values = metricSoup.select('#highwire-highwire-stats-filter-form > div > table > tbody > tr > td:nth-child(%d)' % i)\n",
    "                total = 0\n",
    "                for val in values:\n",
    "                    total += int(val.text.replace(',',''))\n",
    "                entry['engagementMetrics'][sectionTitle] = total\n",
    "            doisProcessed[preprintDOI] = entry['engagementMetrics']\n",
    "            metadata['engagement'][str(int(currentMonthStartTimestamp))].append(entry['doi'])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error here, retrying')\n",
    "            retries += 1\n",
    "\n",
    "\n",
    "with open('metadata.json', 'w') as f:\n",
    "    f.write(json.dumps(metadata, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "48272it [11:23, 70.58it/s]\n"
    }
   ],
   "source": [
    "# Gather engagement tweet metrics from Altmetric data\n",
    "import csv \n",
    "with open('altmetricData.csv',encoding='mac_roman') as f:\n",
    "     reader = csv.DictReader(f, delimiter=',')\n",
    "     for row in tqdm(reader):\n",
    "        if 'DOI' in row:\n",
    "            entry = next((x for x in dataWithPublished if x['doi'] == row['DOI']), None)\n",
    "            if entry is not None:\n",
    "                entry['altmetricData'] = row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the gathered data to the file system\n",
    "import time\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(dataWithPublished,ensure_ascii=False, indent=4, default=str))\n",
    "\n",
    "metadata['lastUpdated'] = time.strftime(\"%m/%d/%Y\")\n",
    "with open('metadata.json', 'w') as f:\n",
    "    f.write(json.dumps(metadata, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {}
   },
   "outputs": [],
   "source": [
    "# Get version 1 abstract\n",
    "\n",
    "v1_pp_abstracts = [datum['abstract'] for datum in dataWithPublished if datum['version'] == '1']\n",
    "published = [datum['published'] for datum in dataWithPublished if datum['published'] != 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {},
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 63954 entries, 0 to 63953\nData columns (total 30 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   doi                               63954 non-null  object \n 1   title                             63954 non-null  object \n 2   authors                           63954 non-null  object \n 3   author_corresponding              63954 non-null  object \n 4   author_corresponding_institution  63954 non-null  object \n 5   date                              63954 non-null  object \n 6   version                           63954 non-null  object \n 7   type                              63954 non-null  object \n 8   license                           63954 non-null  object \n 9   category                          63954 non-null  object \n 10  abstract                          63954 non-null  object \n 11  published                         63954 non-null  object \n 12  server                            63954 non-null  object \n 13  pp_num_authors                    63954 non-null  int64  \n 14  pmid                              58687 non-null  float64\n 15  pmcid                             45402 non-null  object \n 16  pub_date_received                 48684 non-null  object \n 17  pub_date_accepted                 48409 non-null  object \n 18  pub_date_entrez                   58687 non-null  object \n 19  pub_date_pubmed                   58687 non-null  object \n 20  pub_date_medline                  58687 non-null  object \n 21  pub_journal_name                  58687 non-null  object \n 22  pub_num_authors                   58687 non-null  float64\n 23  pub_title                         58687 non-null  object \n 24  pub_abstract                      58339 non-null  object \n 25  engagementMetrics                 63951 non-null  object \n 26  pub_date_revised                  20080 non-null  object \n 27  pub_date_pmc-release              2685 non-null   object \n 28  altmetricData                     16520 non-null  object \n 29  pub_date_retracted                1 non-null      object \ndtypes: float64(2), int64(1), object(27)\nmemory usage: 14.6+ MB\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.DataFrame.from_records(dataWithPublished)\n",
    "a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'10.1371/journal.pone.0106541'"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "published[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit",
   "name": "python_defaultSpec_1600381379662"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}